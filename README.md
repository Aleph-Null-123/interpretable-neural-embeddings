# Interpretable Latent Representations of Neural Activity via Sparse Autoencoders
Project repository for: Interpretable Latent Representations of Neural Activity via Sparse Autoencoders

This project explores how sparse autoencoders can improve the interpretability of neural embeddings generated by CEBRA, a contrastive learning method for neural data. By enforcing sparsity in the latent space, we show that representations become more aligned with behavioral and neuronal structure, while preserving decoding performance.

Detailed written report available in PDF format in this repo.
